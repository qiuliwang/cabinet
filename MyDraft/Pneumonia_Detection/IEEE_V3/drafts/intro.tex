\IEEEPARstart{P}{neumonia} is a very common thoracic disease in our life. In clinical practice, a radiologist needs to consider different source of information to decide on the next treatment plan, as a result, multimodal data plays the key role in decision making process. According to the survey, a radiologist of a major hospital need to diagnosis hundreds of pneumonia cases every day. Thus, developing a fast, robust and accurate CAD system to perform automated diagnosis of pneumonia is meaningful and important. 

There have been several methods and epidemiological studies \cite{Franquet2001Imaging}\cite{Thomas2005Standardized}\cite{deepika2018classification} for pneumonia detection and diagnosing, most of them use image data like chest X-ray as their information source.
Hoo-Chang Shin \cite{Shin2016Learning} combined CNN and LSTM \cite{hochreiter1997long}, proposed a model which could describe the contexts of a detected diseases based on the deep CNN features. This model used CNN to extract features from chest X-Ray and used LSTM to generated MeSH \cite{timmurphy.org} terms for chest X-Ray. In 2017, Xiaosong Wang et al. \cite{Wang2017ChestX} provided hospital-scale chest X-ray database ChestX-ray8 which contained eight common thoracic diseases. This database allowed researchers use deeper neural network to analyze thoracic diseases. They tested different pre-trained CNN models on this dataset. Experiments showed that ResNet50 achieved highest AUROC score 0.6333 in classifying pneumonia. They also provided ChestX-ray14 which contains more kinds of thoracic diseases.
Based on this database, later in 2017, Yao et al. \cite{yao2017learning} achieved 0.713 in AUROC score using DenseNet Image Encoder. Pranav Rajpurkar, Andrew Y. Ng et al. \cite{Rajpurkar2017CheXNet} developed CheXnet with 121 convolutional layers and achieved AUROC 0.7680 in pneumonia classification.
In 2018, Xiaosong Wang et al. \cite{Wang2018TieNet} proposed TieNet, which could classify the chest X-Rays into different diseases and generate the report at the same time. In TieNet, CNN was used capture features of chest X-Rays, RNN learned these features and generated report based on attention mechanism, which could help model to focus on different parts of chest X-Ray alone with the generation of reports. In pneumonia classification problem, they achieved 0.947 in AUROC based on report, but they only achieved 0.917 in AUROC on hand-labeled data. 

Studies above have something in common. First of all, they are designed for chest X-Rays. Chest X-rays used to be the best available method for detect pneumonia, played a crucial role in clinical care\cite{Franquet2001Imaging} and epidemiological studies\cite{Thomas2005Standardized}. However, compared to chest X-rays, CT scans have a clearer view of patients' bodies, since bones, skin, vessels, mediastinal and lung tissues may cause overlapping shadows in chest X-ray and cause misdiagnosis. CT allows visualization of lung structures\cite{korfiatis2009texture}, which can help to diagnosis pneumonia in early stage and avoid delayed treatments.
Each slice of CT scans is a 2-D image of human body scan, besides 2-D visual features from CT, you can also reconstruct 3-D structure of human bodies using these slices. Extensive studies show that 3-D CNN is the best choice for keeping 3-D spatial information in CT\cite{Yorozu1987Electron}. However, 3-D CNN cannot be applied to raw CT data directly since it will bring a heavy burden to the server. Radiologists need to accurately measure the lesions, so we cannot reduce the size of images by resizing at will.
 
Second, these models are not designed following the radiologists' diagnosing process but are designed for the convenience of computer vision studies and deep learning model training. For models like CheXnet, image information is the key of models. Few of them combine image visual features with clinical information. Models like TieNet do combine image visual features with descriptions about images written by radiologists. But these descriptions only provide information related to images, which means no extra clinical information is provided to models. We also believe using descriptions about images written by radiologists to improve models is not quite convincing, since descriptions like `Findings' and `impressions' sometimes include diagnosis conclusions.
Patients' complaints is a very useful information when doctors are diagnosing, since complaints is patients' direct feeling about their physical condition, telling us the patients' pain location, symptoms and how long have they been ill. Moreover, information of age and gender is also related to some certain diseases\cite{xiaojian2011analysis} \cite{huang2014design}. However, as far as we know, few studies use this information to improve CAD systems for pneumonia. 

In general, there are two major drawbacks of existing CAD systems for pneumonia: (1) They cannot handle raw CT scans, which allows visualization of lung structures; (2) Few studies consider clinical information like patients' complaints, which is conflict to clinical practice.

To address such drawbacks, we propose a novel Multimodal Data Diagnosis Network(MDDNet) for Clinical Pneumonia Classification. We use raw data collected from The First Affiliated Hospital of Army Medical University, each case contains not only CT image information, but also clinical information about patient gender, age and complaints. 
In MDDNet, A Recurrent Convolutional Neural Network(RCNN) is used to capture visual features from CT slices. CT allows visualization of lung structures, but lots of redundant information will bring a heavy burden of calculation if we use 3-D CNN. RCNN uses a 2-D CNN to capture visual features from each 2-D slice, and LSTM captures relationships between slices, which means RCNN can analyze 3-D spatial information while reduce the need of calculation. Each CT image will be transformed into a 3-channel image with three windows: Lung Window(LW), High Attenuation(HA) and Low Attenuation(LA). LW provides visual features of normal lung tissues, HA provides visual features of abnormal increase in lung density, LA provides visual features of abnormal decrease in lung density. Three channels complement each other, which not only maintains the ability to extract information from normal lung tissues, but also increases the ability to extract information from abnormal lung tissues.
Besides image data, we also include clinical data in our MDDNet. Complaints can provide location of pain, symptoms and how long have patients been ill. These information is related to CT image and enhance the visual features extracted from CT. Moreover, information about age and gender can provide priori information since patients of different age and gender must have differences in the morphology of thoracic cavity and lungs. We use another LSTM(Long Short Term Memory) to analyze semantics from complaints, information of age and gender will be treated as two extra variables. Our model MDDNet(Multimodal Data Diagnosis Network), as shown in Fig~\ref{MMDD}, will learn a joint distribution of all features above and gives out the final classification result.